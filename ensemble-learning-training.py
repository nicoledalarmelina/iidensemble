# -*- coding: utf-8 -*-
"""organized-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XF3NLNwSbsFcIIK6gH6Gdw-eaIQtTK3P
"""

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score
from imblearn.under_sampling import RandomUnderSampler

from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import xgboost

from sklearn.ensemble import StackingClassifier

import time

import pickle

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import confusion_matrix

"""

---

**Printing the dataset information**"""

def printDatasetInfo(dataset):
  print('Dataset information --------- \n')
  print(dataset.info(memory_usage = "deep"))
  dbInfo = dataset.info(memory_usage = "deep")
  cm_data.write(str(dataset.info(memory_usage = "deep")))
  cm_data.write('\n')
  printRowsCount(dataset)

"""

---

**Reducing dataset size**"""

def reduceDatasetSize(dataset):
  print('Reducing dataset size ---------')
  cm_data.write('Reducing dataset size ---------\n') 
  for column in dataset:
    if dataset[column].dtype == 'float64':
      dataset[column]=pd.to_numeric(dataset[column], downcast='float')
    if dataset[column].dtype == 'int64':
      dataset[column]=pd.to_numeric(dataset[column], downcast='integer')
  return dataset

"""

---

**Getting reduced dataset**"""

def getReducedDataset(dataset):
  return dataset.sample(frac =.1)

"""

---
**Fast mode**
"""

def executeFastMode(dataset):
  print('--------- Starting Fast Mode ---------')
  cm_data.write('--------- Starting Fast Mode ---------\n') 
  printDatasetInfo(dataset)
  reduceDatasetSize(dataset)
  reducedDB = getReducedDataset(dataset)
  printRowsCount(reducedDB)
  return reducedDB

"""

---

**Printing rows count**"""

def printRowsCount(dataset):
  print('Row count is:', len(dataset['Target'] == 0))
  print('Normal flow:', len(dataset[(dataset['Target']==0)]))
  print('Under attack flow:', len(dataset[(dataset['Target']==1)]))
  cm_data.write('Row count is:' + str(len(dataset['Target'] == 0)) + '\n')
  cm_data.write('Normal flow:' + str(len(dataset[(dataset['Target']==0)])) + '\n')
  cm_data.write('Under attack flow:' + str(len(dataset[(dataset['Target']==1)])) + '\n')

"""

---

**Getting Pearson features**"""

def getY(dataset):
  return dataset.iloc[:, 24].values

# Selected features from Pearson correlation coefficient
# 6: DstPort
# 19: SrcLoss
# 20: DstLoss
# 22: PLoss
def getX(dataset):
  cm_data.write('Getting Pearson features ---------\n') 
  cm_data.write('6: DstPort - 19: SrcLoss - 20: DstLoss - 22: PLoss\n') 
  return dataset.iloc[:, [6, 19, 20, 22]].values

"""

---

**Balancing the dataset**"""

def balanceDataset(X, y):
  print('Balancing the dataset ---------')
  cm_data.write('Balancing the dataset ---------\n') 
  rus = RandomUnderSampler()
  X_res, y_res = rus.fit_resample(X, y)

"""

---

**Splitting the dataset**"""

def splitDataset(X, y):
  print('Splitting the dataset ---------')
  cm_data.write('Starting stacking trainning ---------\n') 
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
  return [X_train, X_test, y_train, y_test]

"""

---

**Using Stacking to train the model**"""

def loadAlgorithms():
  dtc =  DecisionTreeClassifier()
  rfc = RandomForestClassifier()
  knn =  KNeighborsClassifier()
  xgb = xgboost.XGBClassifier()

  return [dtc, rfc, knn, xgb]

def startStacking():
  print('Starting stacking trainning ---------')
  cm_data.write('Starting stacking trainning ---------\n')  
  clf = loadAlgorithms()

  return clf

def calculateScore(clf, X_train, y_train):
  start_time = time.time()
  for algo in clf:
    score = cross_val_score( algo,X_train,y_train,cv = 5,scoring = 'accuracy')
    print("The accuracy score of {} is:".format(algo),score.mean())
    cm_data.write("The accuracy score of " + str(format(algo)) + " is: " + str(score.mean()) + "\n")  

  print("Accuracy calculating time in seconds: "+ str((time.time() - start_time))+"\n")
  cm_data.write("Accuracy calculating time in seconds: "+ str((time.time() - start_time))+"\n")

def calculateAccuracyScore(clf, X, y):
  print(clf)
  dtc =  DecisionTreeClassifier()
  rfc = RandomForestClassifier()
  knn =  KNeighborsClassifier()
  xgb = xgboost.XGBClassifier()
  clf = [('dtc',dtc),('rfc',rfc),('knn',knn),('xgb',xgb)] #list of (str, estimator)

  lr = LogisticRegression()
  stack_model = StackingClassifier( estimators = clf,final_estimator = lr)
  score = cross_val_score(stack_model,X,y,cv = 5,scoring = 'accuracy')
  scoreMean = score.mean()
  print("The accuracy score of is:",score.mean())
  cm_data.write("The accuracy score of is:" + str(scoreMean) + "\n")  
  
  return stack_model

def fitModel(X_train, y__train, model):
  model.fit(X_train, y_train)

"""

---

**Evaluating the model**"""

def getScore(classifier_LR, X_test, y_test):
  score = -1*cross_val_score(classifier_LR, X_test, y_test, cv = 10, scoring = 'neg_mean_absolute_error').mean()
  print('Score: ', score)
  cm_data.write("Score: " + str(score) + "\n")

def predictTest(X_test, y_test, classifier_LR):
  start_time = time.time()
  y_pred = classifier_LR.predict(X_test)
  print("Predict time in seconds: "+ str((time.time() - start_time))+"\n")
  cm_data.write("Predict time in seconds: "+ str((time.time() - start_time))+"\n")  
  getScore(classifier_LR, X_test, y_test)
  return y_pred

def generateConfusionMatrix(y_test, logistic_preds):
    cm_data.write('Generating confusion matrix ---------\n')  
    cm = confusion_matrix(y_test, logistic_preds)
    cm_data.write(repr(cm[0,0])+","+repr(cm[0,1])+"\n")
    cm_data.write(repr(cm[1,0])+","+repr(cm[1,1])+"\n")
    cm_data.write('\n')

def evaluateModel(X_test, X_train, y_test, y_train):
  print('Evaluating the model using Logistic Regression --------- ')
  cm_data.write('Evaluating the model using Logistic Regression ---------\n')  
  classifier_LR = LogisticRegression(random_state = 0)
  classifier_LR.fit(X_train, y_train)
  predictTest(X_test, y_test, classifier_LR)
  return classifier_LR

"""

---

**Comparing predictions with actual values**"""

def comparing(classifier_LR, X_test, y_test):
  print('Comparing predictions with actual values --------- ')
  cm_data.write('Comparing predictions with actual values ---------\n')  
  # Gerando as predições
  logistic_preds = classifier_LR.predict(X_test)

  # Criando um dataframe para comparar o valor real com nossas predições
  logistic_comparison = pd.DataFrame()
  logistic_comparison['Valor Real'] = y_test
  logistic_comparison['Predição'] = logistic_preds

  result = logistic_comparison.head(10)
  cm_data.write(str(result) + '\n')  
  return logistic_preds

"""

---

**Saving model to disk**"""

def saveModel(model, filename):
  print('Saving the model --------- ' + str(time.time()))
  cm_data.write('Saving the model ---------\n')  
  pickle.dump(model, open(filename, 'wb'))

"""

---

**Executing**"""

def executingCode(dbName, learningMode):
    cm_data.write('------ Initializing -------\n')
    initializing_time = time.time()
       
    
    if (dbName == 'ddos'):
      cm_data.write('------ Loading DDoS dataset -------\n')  
      dataset = pd.read_csv('dataset-ddos.csv')
    else:
      cm_data.write('------ Loading Reconnaissance dataset -------\n')  
      dataset = pd.read_csv('dataset-reconnaissance.csv')
    
    if (learningMode == 'default'):
      print('--------- Starting Default Mode ---------')
      cm_data.write('--------- Starting Default Mode ---------\n')  
      executionDB = dataset
    else:
      executionDB = executeFastMode(dataset)
    
    y = getY(executionDB)
    X = getX(executionDB)
    balanceDataset(X, y)
    X_train, X_test, y_train, y_test = splitDataset(X, y)
    
    clf = startStacking()
    
    calculateScore(clf, X_train, y_train)
    
    model = calculateAccuracyScore(clf, X, y)
    
    classifier_LR = evaluateModel(X_test, X_train, y_test, y_train)
    logistic_preds = comparing(classifier_LR, X_test, y_test)
    generateConfusionMatrix(y_test, logistic_preds)
    filename = 'ensemble_model_'+ dbName +'.sav'
    saveModel(model, filename)
    print('--------- End ---------')
    cm_data.write('------ End - ' + str((time.time() - initializing_time)) + ' -------\n')  

"""

---

**Initializing**"""

#declarations

#results
X_res = [] 
y_res = []

#algorithms
#dtc
#rfc
#knn
#xgb

#train and test
X_train = []
X_test = []
y_train = []
y_test = []

"""**Choosing the dataset**"""

dbNames = ['ddos', 'reconnaissance'] # ddos | reconnaissance

learningModes = ['fast', 'default'] # fast | default

for dbName in dbNames:
	for learningMode in learningModes:
	    cm_data = open('AlgorithmReport-' + dbName + '-' + learningMode + '.txt', 'w')
	    executingCode(dbName, learningMode)
	    cm_data.close()

